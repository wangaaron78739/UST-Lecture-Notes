\documentclass[../main/main.tex]{subfiles}


\begin{document}

\section{September 10th, 2020}
\subsection{More Linear Algebra Review}
\begin{definition}
    If $T:  \R^{m}\to  \R^{n}$, then the \vocab{image} of $T$ is:  \[
        \text{image} T = \{y \in  \R^{n} : \exists x \in  \R ^{m} \text{ s.t. } y=T(x)\} 
    .\] 
   The $ \text{image}$ 
\end{definition}
\begin{definition}
    Let $T: \R^{m} \to  \R^{n}$ be a linear transformation. The \vocab{rank} of $T$ is the dimension of the image of $T$.
\end{definition}
\begin{lemma}
   If $T: \R^{m}\to  \R^{n}$ is linear, then: 
   \begin{enumerate}
       \item $ \text{rank }T \le n$
       \item $ \text{rank }T \le m$
   \end{enumerate}
\end{lemma}
\begin{proof}
    The proof of 1) is trivial. For 2), we use the theorem below. 
\end{proof}
\begin{theorem}
   \[
   \text{rank }T + \text{ dim ker } T = m
   .\]  
\end{theorem}

\begin{definition}
    A linear transformation $T: \R^{m} \to  \R ^{n}$ has \vocab{full rank} if: \[
    \text{ rank }T = \min \{n,m\} 
    .\] 
\end{definition}

\begin{lemma}
   If $T: \R^{m}\to  \R^{n}$ has full rank, and: 
   \begin{enumerate}
       \item if $m\ge n$, then the matrix of $T$ relative to certain basis is $ \begin{bmatrix} I_n & 0 \end{bmatrix} $
       \item if $m \le  n$, then the matrix of $T$ relative to a certain basis is  $ \begin{bmatrix} I_m \\ 0 \end{bmatrix} $
   \end{enumerate}
\end{lemma}
\begin{example}
    Fix $e\in \R^{n}$, define $T: \R^{n}\to \R $ by $T(x) = \left<x,e\right>$, $ \forall x\in \R^{n}$, then: 
    \begin{itemize}
        \item its rank is 1 if $e\neq 0$
        \item its rank is 0 if $e = 0$.
    \end{itemize}
\end{example}
\begin{definition} 
    In the case we choose $e = e_j$, which is the $j$-th standard basis of  $ \R^{n}$, then the map $T$ above is called the \vocab{projection to the $j$-th coordinate}, symbolically written as $\Pi_j$. 
\end{definition}
\begin{definition}
    For a given $c\in \R$, then the set $ \{x\in \R^{n}: T(x) = c\} $ forms a \vocab{hyperplane}
\end{definition}
\begin{definition}
   Let $T: \R^{m}\to  \R^{n}$ be a linear transformation   whose matrix relative to the standard basis is
   \[
       \begin{bmatrix} t_{11} & t_{12} & \ldots & t_m\\
       \vdots & \ddots & & \vdots \\
   t_{m_1} & t_{m_2} & \ldots & t_{mn}\end{bmatrix} 
   \] then the \vocab{norm} of $T$ is  \[
   \|T\| = \sqrt{\sum_{i,j} t_{ij}^2} = \sqrt{\text{tr}(TT^{\top})} 
   .\]  
\end{definition}
\begin{theorem}
   Let $T: \R^{m} \to  \R^{n}$ be linear. Then for every $x\in \begin{bmatrix} x_1\\ \vdots\\x_m \end{bmatrix} \in  \R^{m}$, then:  
   \[
       \|T(x)\|\le \|T\|\|x\|
   .\] 
\end{theorem}
\begin{proof}
    Denote the matrix $T$ relatie to the standard basis as above. If $x \in \R^{m}$, we have: 
\begin{align*} 
    \|T(x)\|^2 &= \left\|T\left( \sum_{i=1}^{m} x_ie_i \right)\right \|^2 \\ &= \left\|\sum_{i=1}^{m} x_iT(e_i)\right\|^2 \\
    &= \left\| \sum_{i=1}^{m} \sum_{j=1}^{n} x_i  t_{ij}e_j \right\|^2\\
    &= \sum_{j=1}^{n} \left( \sum_{i=1}^{m} x_it_{ij} \right)^2  \\
    &\le \sum_{j=1}^{n} \left( \left( \sum_{i=1}^{m} x_i^2 \right) \left( \sum_{i=1}^{m} t_{ij}^2 \right) \right) \quad \text{Cauchy-Schwarz} \\
    &=\sum_{j=1}^{n} \sum_{i=1}^{m} t_{ij}^2 \|x\|\\
    &= \|T\|\|x\| \\
\end{align*}
\end{proof}
\begin{lemma}
    \begin{enumerate}
        \item  If $\|T\|=0$, then $T=0$
        \item If $T,S: \R^{m}\to  \R^{n}$ are linear, then: $\|T+S\| \le  \|T\| + \|S\|$
        \item If $T: \R^{m} \to  \R^{n}$, $S: \R^{\ell} \to \R^{m} $ are linear, then $\|TS\| \le \|T\|\|S\|$
    \end{enumerate}
\end{lemma}
\begin{definition} 
    For a given sequence $ \{a_n\} $ of real numbers, $\{a_n\} $ converges to $a\in \R$ if for every $\epsilon > 0$, there exists $N \in  \N $, such that $n>N$ implies that $|a_n-a|<\epsilon$ 
\end{definition}
\begin{definition}
    For a given sequence $ \{v_n\} $ of vectors in $ \R^{n}$, $\{v_n\} $ converges to $v\in \R^{n}$ if for every $\epsilon > 0$, there exists $N \in  \N $, such that $n>N$ implies that $|v_n-v|<\epsilon$ 
\end{definition}
\begin{definition}
    For a given sequence $ \{T_n\} $ of linear transformations $ \R^{\ell} \to  \R^{n}$, $\{T_n\} $ converges to $T$ if for every $\epsilon > 0$, there exists $N \in  \N $, such that $n>N$ implies that $\|T_n-T\|<\epsilon$ 
\end{definition}

\begin{lemma}
    Let $ \{v_n\} $ be a sequence in $ \R^{m}$ which converges to $v\in \R^{m}$. Then: $
        \forall j \in  \{ 1,\ldots,m\}, \quad \{ \Pi_j(v_n)\}   
    $ converges to $\Pi_j(v)$
\end{lemma}
\begin{proof}
    Since $\{v_n\} $ converges, $\forall \epsilon>0$, $\exists N\in \N$ s.t. if $n>N$, then $ \|v_n-v\|<\epsilon>$, Thus if $n>N$, then $|\Pi_j(v_n)-\Pi_j(v) = |\Pi_j(v_n-v)$
    For every $\epsilon>0$, there exists $N\in \N$ such that if $n>N$, then 
    if $n>N$, then $|\Pi_j(v_n) - \Pi_j(v)|\le \|v_n-v\| < \epsilon$
\end{proof}
\begin{lemma}
    Let $ \{v_n\} $ be a sequence in $ \R^{m}$ so that $ \forall j\in \{1,..,m\} $, $\{\Pi_j(v_n)\} $ converges. Then $\{v_n\} $ is a convergent sequence of vectors. 
\end{lemma}
\begin{proof}
    Suppose that $ \{\Pi_j(v_n)\} $ converges to $a_j$ and let  $v = \begin{bmatrix} a_1 \\ \vdots \\ a_m\end{bmatrix} $. Thus if $n>N_j$, then  $|\Pi_j(v_n-v)|<\frac{\epsilon}{m}$
    if  $n>N = \max \{N_1,N_2, N_m\}  $, we have: \[
        \|v_n-v\| < \sum_{j} |\Pi_j(v_n-v)| < \frac{\epsilon}{m} \times  m = \epsilon
    .\]  
    Since 
\end{proof}
       \end{document}

