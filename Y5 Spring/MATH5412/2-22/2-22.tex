    \documentclass[../main/main.tex]{subfiles}

\begin{document}

\section{February 22nd, 2022}
\subsection{Functional Limit Theorems Continued}
\begin{theorem}[\vocab{Cramer-Wold device}]
	A sufficient condition for $X_n \implies X_{\infty}$ is that: \[
		\theta \cdot X_n \implies \theta \cdot X_{\infty}
	\] for any given $\theta \in \R^{k}$.
\end{theorem}
\begin{proof}
	According to the continuity theorem, we just need to show that $\varphi_n(t) \to \varphi_{\infty}(t)$. Thus, if we choose $t = \theta$, then this is true for any $t$.
\end{proof}

\begin{theorem}[\vocab{multivariate CLT}]
	Let $X_1, X_2, \ldots $ be i.i.d. random vectors in $\R^{k}$ with \[
		\E X_1 = \mu = \begin{pmatrix}
			\mu_1 \\ \vdots \\ \mu_k
		\end{pmatrix}, \quad \Gamma_{ij} = \E (X_n(i) - \mu_i) (X_n(j) - \mu_j),
	\]  If $S_n = \sum\limits_{i=1}^{n} X_i$, then: \[
		\frac{1}{\sqrt{n}}(S_n - n \mu )\implies \chii
	\] which is the multivariate Gaussian $N(0,\Gamma)$.
\end{theorem}

\begin{definition}[\vocab{multivariate Gaussian}] The j.p.d.f. is: \[
		(2 \pi)^{-\frac{k}{2}} \operatorname{det}(\Gamma)^{-\frac{1}{2}} \exp(-\frac{1}{2} X^{T}\Gamma X)
	\]
	and the c.f. is: \[
		\E e^{it X} = \exp(-\frac{1}{2}t^{T}\Gamma t)
	\]
\end{definition}

\begin{proof}
	WLOG, we assume $\mu = 0$ by considering $X_n' = X_n - \mu$. Let $\theta \in \R^{k}$, meaning $\theta \cdot X_n$ is a r.v. We have: \[
		\E \theta \cdot X_n = 0, \quad \E (\theta \cdot X_{n})^2 = \theta^{T} \Gamma \theta
	\] By 1D CLT: \[
		\frac{\sum\limits_{i=1}^{n}\theta \cdot X_n}{\sqrt{n}} \implies \theta \cdot \chii
	\]
	By the Cramer-Wold device, we have: \[
		\frac{S_n}{\sqrt{n}} \implies \chii.
	\]
\end{proof}

Now, we will return to a functional space. For this, let us review some common functional spaces:

\begin{definition}[$C \begin{bmatrix}0,1\end{bmatrix}$ Functional Space]
	Similar to random variables, we define the space and metric:
	\begin{description}
		\item[space:]  all continuous functions on $[0,1]$.
		\item[metric/topology:] uniform topology $\rho(x,y) = \sup_{0 \leq t \leq 1}|x(t)-y(t)|$.
		\item[Borel $\sigma$-field:] generated by open sets.
	\end{description}
	For notation, we have: $C=C[0,1]$, $\mathscr{C} = \mathscr{B}(C[0,1])$.
\end{definition}

\begin{definition}[random function in $C$]
	Given a probability space $(\Omega, \mathscr{F} , \mathbb{P})$, we say a map $X : \Omega \to C$ a random function if: \[
		X^{-1}(A) = \{\omega : X(\omega) \in A\} \in \mathscr{F} , \quad \forall A \in \mathscr{C}.
	\]
\end{definition}
\begin{remark}
	Some other possible notations include $X_t(\omega)$, $X(t;\omega)$, $t \in[0,1]$.
\end{remark}
For a fixed $\omega$, we call $X(\cdot; \omega)$ the \vocab{trajectory/sample path/realization of $X$}.

We can first think of $X(t)$ as a measurable map: \[
	(\Omega, \mathscr{F},) \overset{X(t)}{\to} (C, \mathscr{C})
\] To consider it as a probability measure, we have the push forward/induced measure, giving us the distribution of $X(t)$.
\begin{definition}[Distribution of $X(t)$]
	If we think the random map as the map between two probability space: \[
		(\Omega, \mathscr{F},\mathbb{P}) \overset{X(t)}{\to} (C, \mathscr{C}, \underbrace{\mathbb{P} \circ X^{-1}}_{\mu_X})
	\] with: \[
		\mu_X(A) = \mathbb{P} \circ X^{-1}(A) = \mathbb{P}(X^{-1}(A)), \quad A \in \mathscr{C}.
	\]
	This defines a probability measure in the new measurable space.
\end{definition}

This is a bit more abstract than probability measures on $\R$ or $\R^{k}$, since for the simpler spaces, we have much simpler ways to identify them.
For example, for $\R$, using the Stieltjes measure function, if we have a Stieltjes distribution, we don't need to know the value of the distribution on all Borel sets, we only need on all half-lines. For $\R^{k}$, we can use the joint distribution function. Naturally, we ask if we can identify the $\mu_X$ in $C[0,1]$. In other words, we don't want to find $\mu_X$ on all $A$, but only on some special class of events. To do this, we will first introduce the finite-dimensional distribution on $\mu_X$.

\subsection{Finite-Dimensional Distribution}
The general idea is to pick a few time points on the sample path and consider their joint distributions.

\begin{definition}[natural projection]
	The natural projection $\Pi_{t_1, \ldots t_k}: C \to \R^{k}$:
	\[
		\Pi_{t_1, \ldots t_k}  = X(t) \to (X(t_1), \ldots X(t_n)), \quad t \in [0,1]
	\]
	with $\R^{k}$ being equipped with the usual Euclidean metric.
\end{definition}
Since this is a continuous map, it is also measurable. With this, we can induce a probability measure.
\[
	(C, \mathscr{C}, \mu_X)\overset{\Pi_{t_1, \ldots t_k} }{\to} (\R^{k}, \mathscr{B}(\R^{k}), \mu_X \circ \Pi_{t_1, \ldots t_k} ^{-1})
\]
Meaning $\mu_X \circ \Pi_{t_1, \ldots t_k} ^{-1}$ is a finite dimension distribution, with: \[
	\mu_X \circ \Pi_{t_1, \ldots t_k} ^{-1}(B_1 \times B_2 \times \ldots \times B_k) = \mu_X (\Pi_{t_1, \ldots t_k} ^{-1}(B_1 \times B_2 \times \ldots \times B_k) )
\]
\begin{remark}
	Since $ \mu_X \circ \Pi_{t_1, \ldots t_k} ^{-1} $ is on $\R^{k}$, we only need to consider it on rectangles.
\end{remark}

As such, given $\mu_X$, we can get the probability distribution. However, what we're more interested in is the reverse, i.e. given $\mu_X \circ \Pi_{t_1, \ldots t_k} ^{-1}$, fix a $\mu_X$ on $C$.\\
Another view of this finite-dimensional distribution is considering: \[
	(\Omega, \mathscr{F},\P ) \overset{X_{t_1, \ldots,t_k} = (X(t_1), \ldots, X(t_k))}{\to} (\R^{k}, \mathscr{B}(\R^{k}), \P \circ X^{-1}_{t_1, \ldots,t_k})
\] Now, we have: \[
	\P \circ X^{-1}_{t_1, \ldots,t_k}(B_1 \times B_2 \times \ldots \times B_k) = \P((X(t_1),\ldots,X(t)) \in B_{1} \times \ldots B_k)
\]
\[
	\implies \P \{\omega : (X(t_1;\omega)\ldots X(t_k;\omega)) \in B_1 \times \ldots \times B_k\}
\]
This gives us a way to get the finite dimensional distribution even if we don't know $\mu_X$. Now our goal is to choose arbitrary fixed points $t_k$ and fix the $\mu_X$.\\

Suppose we have a collection of distributions $\nu_{t_1, \ldots , t_k}$ on $\R^{n}$ for all $t_1, \ldots , t_k \in [0,1]$, and all $k \in \N$, we want to know when we can regard them as the class of finite-dimensional distribution  of a measure $\nu$ on $(C,\mathscr{C})$ and whether they can uniquely describe $\nu$. Let us first find some requirements for this to be a finite-dimensional distribution:

\begin{enumerate}
	\item
	      If $\nu_{t_1, \ldots ,t_k}$'s are indeed finite-dimensional distributions of some $\mathcal{B}$, then: \[
		      \nu_{t_1, \ldots ,t_k}(B_1 \times\ldots \times B_k) = \P ((X(t_1),\ldots X_(t_k)) \in B_1 \times \ldots B_k)
	      \] for some random function $X(t)$ with distribution $\nu$. Then: \[
		      \nu_{t_1, \ldots ,t_k}(B_1 \times \ldots\times B_k) = \nu_{\Pi(t_1), \ldots ,\Pi(t_k)}(B_{\Pi(t_1)} \times \times B_k) ,
	      \] where $\Pi$ is any permutation of 1 to $k$.
	\item We should also be able to add to $k$ and maintain consistency:\[
		      \nu_{t_1, \ldots ,t_k}(B_1 \times\ldots \times B_k) =  \nu_{t_1, \ldots ,t_k, t_{k+1}}(B_1 \times\ldots \times B_k \times \R).
	      \]
\end{enumerate}

It turns out that these two consistency conditions are enough.

\begin{theorem}[\vocab{Kolmogorov extension theorem}]
	Let $T \in [0,1]$ be some interval. For any finite sequence of distinct time $t_1, \ldots t_k \in T$. Let $\mu_{t_1, \ldots t_k}$ be a probability measure on $\R^{k}$. Suppose these measures satisfy the two consistency conditions:
	\begin{enumerate}
		\item $ \nu_{\Pi(t_1), \ldots ,\Pi(t_k)}(B_{\Pi(t_1)} \times \times B_k) =\nu_{t_1, \ldots ,t_k}(B_1 \times \ldots\times B_k) $
		\item $ \nu_{t_1, \ldots ,t_k, t_{k+1}}(B_1 \times\ldots \times B_k \times \R)=\nu_{t_1, \ldots ,t_k}(B_1 \times\ldots \times B_k)$,
	\end{enumerate}
	Then there exists some probability space $(\Omega, \mathscr{F}, \P)$ and a random function $X: (\Omega, \mathscr{F}) \to (C, \mathscr{C})$ such that: \[
		\nu_{t_1, \ldots ,t_k}(B_1 \times\ldots \times B_k) = \P((X(t_1)\ldots X(t_k)) \in B_1 \times \ldots \times B_k)
	\] and the distribution of $X(t)$, i.e. $\nu$ is uniquely determined by $\nu_{t_1, \ldots t_k}$.
\end{theorem}

\begin{proof}
	For proof, see Billingsley \cite{Bill86}. In the book it is called Kolmogorov existence theorem.
\end{proof}
This means that it is enough to know all the finite-dimensional distributions.


\subsection{Weak Convergence in $(C,\mathscr{C})$}
let us consider random functions: $X_1(t), \ldots , X_n(t) \in C$ with distributions $\mu_1, \ldots , \mu_n$.

\begin{definition}[\vocab{weak convergence}]  We say $\mu_n \implies \mu$ or $X_n(t) \implies X(t)$ for some $X(t) $ with distribution $\mu$ if: \[
		\E f(X_n) \to \E f(X)
	\] for any bounded and continuous functional. Here bounded means $\sup\limits_{X \in C} |f(X)| \leq M$ for some $M$.
\end{definition}

\begin{theorem}[\vocab{continuous mapping theorem}] If $h : C \to \R$ is continuous. Then $X_{n} \implies X$ implies $h(X_n) \implies h(X)$, with $X_n$ being a random function.
\end{theorem}
\begin{proof}
	Let $g: \R \to \R$ be any bounded continuous function. Then, $g \circ h$ is again a bounded continuous functional. If $X_n \implies X$, then: \[
		\E g(h(X_n)) \implies \E g(h(X))
	\] by definition of. Thus, $h(X_n) \implies h(X)$.
\end{proof}
\begin{example}
	If $X_n \implies X$ in $C$, then $ \sup_{t \in [0,1]} X_n(t) \implies \sup_{t \in [0,1]} X(t) $ using triangular inequality.
\end{example}
\begin{example}
	If $X_n \implies X$ in $C$, then $\int\limits_{0}^{1} X_n(t) dt \implies \int\limits_{0}^{1} X(t) dx$.
\end{example}

Now, our task is to prove weak convergence in $C$. We first need prove the finite-dimensional convergence and tightness.

\begin{definition}[finite-dimensional convergence]
	We say that the finite-dimensional convergence holds for $X_n \implies X$ if for any given $t_1, t_2, \ldots t_k$ and any $k \in \N$, there is: \[
		(X_n(t_1) ,\ldots, X_n(t_k)) \implies (X(t_1), \ldots , X(t_k)).
	\]
\end{definition}

\begin{remark}
	Finite-dimensional convergence is not convergence-determining in $C$. For example, let us consider: \[
		Z_n(t) = nt \ind(t \in [0, 1 / n]) + (2-nt) \ind (t \in [1 / n , 2 / n])
	\]
	we define random function: \[
		X_n(t; \omega) = Z_n(t) , \quad \forall \omega \in \Omega
	\] hence, $\P (X_n(t) = Z_n(t)) = 1$.
	Let $X(t; \omega) = 0, \,\, \forall \omega, \forall t$. For any given $t_1, \ldots t_k \in [0,1]$: \[
		(X_n(t_1) ,\ldots, X_n(t_k)) \implies (X(t_1), \ldots , X(t_k))
	\] However, $X_n(t)  \centernot\implies X(t)$, otherwise: \[
		\sup_{t \in [0,1]} X_n(t) = 1 \implies \sup_{t \in [0,1]} X(t) = 0
	\] by continuous mapping theorem.

\end{remark}

Note that for any fixed $n$, the finite-dimensional distribution of $(X_n(t_1), \ldots , X_{n}(t_k))$'s can be used to determine the distribution of $X_n(t)$. However, here $t_1 , \ldots , t_k$ can also be in the interval $(0, 2 / n]$. But in finite-dimensional convergence, $t_1, \ldots t_k$ are given at the beginning, i.e. independent of $n$, meaning it can jump out of the interval.

\end{document}

