    \documentclass[../main/main.tex]{subfiles}

\begin{document}

\section{March 3rd, 2022}
For this chapter, we will start by rigorously introducing conditional expectation, and then move on to martingale theory.
\subsection{Conditional Expectation}
In modern probability, we first give the conditional expectation in a sigma field, and then note that other cases are special cases of this. Here we will go the opposite direction. The most general definition is not constructive, meaning we usually can't write out an explicit formula, but it still maintains special properties that are useful.\\

Recalling the basic level of conditional expectation, considering a probability space $(\Omega, \mathscr{F}, \P)$, with random variable $X = (\Omega, \mathscr{F}) \to (\R , \mathscr{B})$, the conditional expectation of $X$ given an event $A$ ($\in \mathscr{F}$, with $\P(A) > 0$) is: \[
	\E(X | A) = \frac{\E X \ind_A}{\P(A)} = \frac{\int_{A} X d \P}{\P(A)}.
\]
\begin{remark}
	We can write: \[
		\E X =  \frac{\E X \ind_{\Omega}}{\P(A)}.
	\]
\end{remark}
\begin{example}
	The conditional probability of $B$ given $A$, is: \[
		\P(B | A)= \E(\ind_B | A)  = \frac{\E \ind_B \cdot \ind_A}{\P(A)} = \frac{\P(A \cap B)}{\P(A)}.
	\]
\end{example}
\begin{example}
	The conditional expectation of $X$ given $Y=y$ (i.e. $Y^{-1}(\{y\}) \in \mathscr{F}$), is: \[
		\E[X | Y = y] = \psi(y)
	\] since it is a function of $y$. This leads to a second level of conditional expectation.
\end{example}

The conditional expectation of $X$ given $Y$ is: \[
	\E[X | Y] = \psi (y)
\] which is a random variable.

\begin{remark}
	The conditional expectation of a variable given an event is a number, the conditional expectation of a variable given a variable is a random variable.
\end{remark}
To motivate this idea, consider the following example:
\begin{example}
	Consider the probability space: \[
		\Omega = \{1, 2, 3, 4, 5, 6\}, \quad \mathscr{F} = 2^{\Omega}, \quad \P(\{i\}) = \frac{1}{6}.
	\] Let $X(\omega) = \omega$ and $Y(\omega) = \begin{cases}
			1 , & \omega \in \{2, 4, 6\} \\
			0,  & \text{ otherwise}
		\end{cases}$.
	\begin{itemize}
		\item The expectation $\E X$ is the "best guess" of $X$ without any information. In this case, for example we might case about the mean square error \[
			      \argmin_e \E (X-e)^2 = \E X = 3.5.
		      \]
		\item The conditional expectation given event $Y=y$ ($\E[X | Y=y]$) is the "best guess" given info $Y=y$. For example: \[
			      \E[X | Y=1] = \frac{\E X \ind_{Y = 1}}{\P(Y=1)} = 2 \cdot \frac{1}{6} ( 2+4+6) = 4.
		      \] On the other hand: \[
			      \E[X | Y=0] = \frac{\E X \ind_{Y = 0}}{\P(Y=0)} = 2 \cdot \frac{1}{6}(1 +3 +5) = 3.
		      \] one way of thinking is we are allowed to change our guess given new information.
		\item The conditional expectation given $Y$ ($\psi(Y)$) is given by: \[
			      \psi(Y) = 4 \ind (Y=1) + 3 \ind( Y=0).
		      \] In this case, we will still be given the information of $Y$, but we are not allowed to change our guess. Instead, we are allowed to have a "strategy" of future information you'll provide. In other words, this is the "best guess" based on the future information of $Y$.
	\end{itemize}
\end{example}
\begin{example}
	We have $\E[X | X] = X$, since the best guess based on getting the value of $X$ is itself. On the other hand, $\E[X | Y] = \E X$ if $X \perp Y$, since the future information is useless.
\end{example}
Going beyond these two levels, we will define the conditional expectation given a $\sigma$-field $\mathcal{H} \subseteq \mathscr{F}$. If we go back to how we explained the first two examples, the "information" generated by $Y$ is $\sigma(Y)$. In the previous example, this would be $\{\emptyset, \Omega, \{2,4,6\}, \{1,3 5\}\}$. If $\mathcal{H} = \sigma(Y)$, then we shall have: \[
	\E[X | \sigma(Y)] = \E[X | Y].
\] We will define this by looking at the properties of the above definition.
\begin{itemize}
	\item The first property we see is that $\E[X | Y]$ is $\sigma(Y)$measurable, since the information contained cannot be more than that of $\sigma(Y)$.
	\item $\E[X | Y]$ is a constant on any atoms $A \in \sigma(Y)$, since we cannot differentiate items within the atoms.
	      \begin{definition}[atom of a $sigma$-field] There does not exist any nontrivial event $D$ s.t. $D \subset A$
	      \end{definition}
	      This also means: \[
		      \E[\E[X | Y]] \ind_A = \E[X | Y = Y(\omega)]\E \ind_A = \E X \ind_A,
	      \] Since this is true for the atoms, by linearity of expectation, it is true for any event: \[
		      \E [\E[X | Y]]\ind_E = \E X \ind_E ,\quad \forall E \in \sigma(Y)
	      \]
\end{itemize}
Replacing $\sigma(Y)$ by a general $\mathcal{H}$, we have the following definition:

\begin{definition} The conditional expectation of $X$ given $\mathcal{H}\subseteq\mathscr{F}$ is defined as any random variable $Z$ satisfying:
	\begin{enumerate}
		\item $Z$ is $\mathcal{H}$-measurable
		\item $\E Z \ind_A = \E X \ind_A$ for all $A \in \mathcal{H}$.
	\end{enumerate}
\end{definition}

\begin{example}
	Suppose $X$ is $\mathcal{H}$-measurable. We have: $ \E[X | \mathcal{H}] = X.$
\end{example}
\begin{example}
	Suppose $X$ is independent of $\mathcal{H}$, i.e. $X^{-1}(B) \perp A$ for all $B \in \mathscr{B}(\R)$ and $A \in \mathcal{H}$. We have $\E[X | \mathcal{H}] = \E X$.
\end{example}

Thus, we arrive at the definition of conditional expectation:
\begin{definition}
	$\E[X | Y] = \E[X | \sigma(Y)]$. For an event, given an event $A \in \mathscr{F}$, let $\mathcal{H} = \{\emptyset, \Omega, A , A^{C}\}$, we have: \[
		\E[X | \mathcal{H}] = \frac{\E X \ind_A}{\P(A)} \cdot \ind_A + \frac{\E X \ind_{A^{C}}}{\P(A^{C})} \cdot \ind_{A^{C}}
	\] If we observe event $A$, then this reduces.
\end{definition}

Now, we will consider the existence and uniqueness. First, for uniqueness, we will show that any two variables that satisfy these two properties are the same almost surely. Suppose we have $Z$ and $\tilde{Z}$ which both satisfy 1 and  2. For any $\epsilon > 0$, let: \[
	A_\epsilon = \{\omega : Z(\omega) - \tilde{Z} (\omega) > \epsilon \} \in \mathcal{H}
\] by property 1. By property 2, we have: \[
	0 = \int\limits_{A_\epsilon} (X - X) d\P = \int\limits_{A_{\epsilon}} (Z - \tilde{Z}) d\P \geq \epsilon \P(A_{\epsilon}) \implies \P(A_{\epsilon}) = 0 \implies Z = \tilde{Z} \text{ a.s.}
\]

Now we will consider the existence.

\begin{definition}
	A measure $\nu$ is absolutely continuous w.r.t $\mu$, written as $\nu \ll \mu$ if $\mu(A) = 0 \implies \nu(A) = 0$.
\end{definition}

\begin{example}
	Consider $\mu$ be the Lebesgue measure and $\nu$ be a distribution of a continuous r.v. Then: \[
		\nu(A) = \int\limits_{A} f d\mu
	\] meaning that $\nu\ll \mu$.
\end{example}

\begin{theorem}[\vocab{Radon-Nikodym Theorem}]
	Let $\mu$ and $\nu$ be $\sigma$-finite measures on $(\Omega, \mathscr{F})$. If $\nu \ll \mu$, then there is a function $f$ which is $\mathscr{F}$-measurable, and for all $A \in \mathscr{F}$: \[
		\nu(A) = \int\limits_{A} f d\mu,
	\] with $f$ being the \vocab{Radon-Nikodum derivative} often denoted by $\frac{d \nu}{d \mu}$.
\end{theorem}
Now we will prove the existence of $\E[X | \mathcal{H}]$:
\begin{proof}
	Let the original probability space be $(\Omega, \mathscr{F} , \P)$.
	Consider $X \geq 0$. Let $\mu = \P$ and set: \[
		\nu(A) = \int\limits_{A} X d\P = \E X \ind_A , \quad \forall A \in \mathcal{H}
	\] apparently $\nu \ll \mu$. By R-N theorem, we know that $\frac{d \nu}{d \mu}$ is $\mathcal{H}$-measurable and: \[
		\nu(A) = \int\limits_{A} X \d \mu = \E X \ind_A
	\] but also: \[
		\nu(A) = \int\limits_{A} \frac{d \nu}{ d \mu} d \mu  = \E[\E[X|\mathcal{H}]]\ind_A
	\]
\end{proof}
\subsection{Properties of Conditional Expectation}
Some of the main properties of conditional expectation can be categorized into those related to measurability, and those related to independence.
\subsubsection{Pulling out independent factors}
\begin{itemize}
	\item If $X\perp \mathcal{H}$, $\E[X|\mathcal{H}] = \E X$.
	\item If $X$ is independent of $\sigma(Y, \mathcal{H})$, $\E[XY | \mathcal{H}] = \E X \E [Y | \mathcal{H}]$. (note needs both independent of $Y$ and $\mathcal{H}$).
	\item If $X \perp Y$, $\mathcal{G} \perp \mathcal{H}$, $X \mathcal{H}$, $X\mathcal{G}$: \[
		      \E[\E[X Y | \mathcal{G}] | \mathcal{H}] = \E X \E Y = \E[\E[XY|\mathcal{H}]|\mathcal{G}].
	      \]
	      \begin{proof}
		      Note that $\E[XY | \mathcal{G}]$ is $\mathcal{G}$-measurable, so $\E[XY|\mathcal{G}] \perp \mathcal{H}$. The proof then follow using the law of total expectation, $\E[\E[XY | \mathcal{G}]] = \E[XY]$.
	      \end{proof}
\end{itemize}
\subsubsection{Pulling out known factors}
\begin{itemize}
	\item If $X$ is $\mathcal{H}$-measurable, $\E[X|\mathcal{H}] = X$.
	\item $\E[f(X) | X] = f(X)$.
	\item If $X$ is $\mathcal{H}$-measurable, then $\E[XY | \mathcal{H}] = X \E[Y | \mathcal{H}]$.
	\item $\E[f(X) Y | X] = f(X) \E[Y | X]$.
\end{itemize}

\end{document}

