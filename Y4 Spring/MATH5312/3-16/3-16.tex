\documentclass[../main/main.tex]{subfiles}


\begin{document}

\section{March 16th, 2021}
\subsection{CG as a Direct Method}
As proved before, GC will get the exact solution after t most $n$ steps. In addition, the complexity per step is: \[
  1 \text{ matrix-vector product }+ \text{opeartions of }O(n)
\]

Note that one matrix vector product is $O(m+n)$ where $m$ is the number of nonzero entries in $A$. This means that the total computational cost is $O(mn+n^2)$ in the worse case.\\

\begin{itemize}
  \item If $A$ is the 1D Discrete Laplacian matrix, this is no better than Cholesky decomposition, which is $O(n)$.
  \item However if $A$ is the 2D Discrete Laplacian, both are $O(n^2)$.
\end{itemize}

\subsection{GC as an Iterative Method}
CG can give a very accurate solution even if $k \ll n$.
\begin{theorem}\label{3-16-th}
  Assume $A$ is SPD. Then $\{ x_{k}\}$ generated by CG satisfies:
  \begin{enumerate}
    \item If $A$ has only $s$ distinct eigenvalues, then: \[
          x_{k} = x_{*} \text{for all $k\geq s$.}
          \]
    \item For a genera $A$: Let $\gamma = \frac{\lambda _{\max}(A)}{\lambda _{\min }(A)} $ be the condition number, then we have: \[
\|x_{k}-x_{*}\|_{A} \leq 2 \left(\frac{\sqrt{\gamma }-1}{\sqrt{\gamma }+1} \right)^{k} \|x_{0}-x_{*}\|_{A}.
          \]
    \item If eigenvalues of $A$ satisfies: \[
          0< \lambda_{1}\leq  \ldots \leq  \lambda _{s} \leq \alpha  \leq \lambda _{s+1} \leq  \ldots  \leq  \lambda _{n-t} \leq \beta  \leq  \lambda _{n-t+1} \leq  \ldots \leq  \lambda _{n}
          \]
          Where $\alpha $ is close to $\beta $, (i.e. most eigenvalues are close together barring $s$ small and $t$ large outlying eigenvalues), then: \[
\|x_{k}-x_{*}\|_{A} \leq  2 \left(\frac{\sqrt{\beta / \alpha }-1}{\sqrt{\beta / \alpha }+1} \right)^{k-s-t} \left(\max_{\lambda  \in [\alpha, \beta]} \prod_{\ell \in \{1, \ldots , t\}\cup \{n-t+1, \ldots  , n\}} \left|\frac{\lambda -\lambda_{\ell } }{\lambda _{\ell }}  \right|\right)
          \] Note that the right factor is a constant.
  \end{enumerate}
\end{theorem}
\begin{corollary}
  From Theorem \ref{3-16-th} (2), we have that the convergence speed depends on $O(\sqrt{\gamma })$, where as for steepest descent, it is $O(\gamma)$, meaning that the CG is much faster than steepest descent.
\end{corollary}
\begin{example}
If $A = (I+vv^{T})$, then there are only two distinct eigenvalues, meaning that CG will converge in only two steps.
\end{example}
\begin{proof}
  By the optimality of CG, we have:
  \begin{align*}
    \|x_{k}-x_{*}\|_{A} &= \min_{x\in x_{0}+K_{k}}\|x_{*}-x\|_{A}\\
                        &= \min_{c\in \RR ^{k}} \left\|x_{*}- (x_{0}+ \sum_{j=0}^{k-1} c_{j}A^{j}r_{0})\right\|_{A}\\
                        &= \min_{c\in \RR ^{k}} \left\|(x_{*}-x_{0})+ \sum_{j=0}^{k-1} c_{j}A^{j+1}(x_{*}-x_{0})\right\|_{A}\\
                        &= \min_{c\in \RR ^{k}} \left\|\left(I +  \sum_{j=1}^{k} c_{j-1}A^{j}\right)(x_{*}-x_{0})\right\|_{A}\\
                        &= \min_{p\in \mathbb{P}_{k}, p(0)=1} \left\|p(A)(x_{*}-x_{0})\right\|_{A}\\
                        &\leq \left( \min_{p\in \mathbb{P}_{k}, p(0)=1} \|p(A)\|_{A}\right) \left\|(x_{*}-x_{0})\right\|_{A}\\
                        &= \left( \min_{p\in \mathbb{P}_{k}, p(0)=1} \|p(A)\|_{2}\right) \left\|(x_{*}-x_{0})\right\|_{A}
    .\end{align*}
  Where $\mathbb{P}_{k}$ is the set of polynomial of degree $k$.\\

  Since $A$ is symmetric, $p(A)$ is also symmetric. Thus, we have:
\begin{align*}
    \|x_{k}-x_{*}\|_{A} &\leq  \left( \min_{p\in \mathbb{P}_{k}, p(0)=1} \|p(A)\|_{2}\right) \left\|(x_{*}-x_{0})\right\|_{A}\\
  &=\left( \min_{p\in \mathbb{P}_{k}, p(0)=1}\max_{i \in \{1,\ldots  , n\}} |p(\lambda_{i})|\right) \left\|(x_{*}-x_{0})\right\|_{A}
  .\end{align*}
\begin{enumerate}
  \item If $A$ has only $s$ distinct eigenvalues, say $\lambda _{1}, \ldots  , \lambda _{s}$, we have: \[
 \min_{p\in \mathbb{P}_{k}, p(0)=1}\max_{i \in \{1,\ldots  , n\}} |p(\lambda_{i})| \leq  \max_{i \in  \{1, \ldots  , n\} }|q(\lambda_{i})| \quad  \forall q \begin{cases}
   q \in \mathbb{P}_{k}  \\
   q(0) = 1
 \end{cases}
        \]
        Let us choose $q$ by: \[
        q(\lambda) = \prod_{i=1}^s \left(\frac{\lambda _{i}-\lambda }{\lambda _{i}} \right)
        \]
        We have check that $q \in \mathbb{P}_{s}\subset \mathbb{P}_{k}$ and that $q(0)=1$. With this, we have: \begin{align*}
                                                                                                                 \min_{p\in \mathbb{P}_{k}, p(0)=1}\max_{i \in \{1,\ldots  , n\}} |p(\lambda_{i})|& \leq  \max_{i \in  \{1, \ldots  , n\} }|q(\lambda_{i})|\\
                                                                                                                 &= \max_{i \in \{ i, \ldots  , s\}} |q(\lambda_{i})| = 0 .\end{align*}
  \item We relax the estimation by:
        \begin{align*}
          \|x_{k}-x_{*}\|_{A} &\leq \left( \min_{p\in \mathbb{P}_{k}, p(0)=1}\max_{i \in \{1,\ldots  , n\}} |p(\lambda_{i})|\right) \left\|(x_{*}-x_{0})\right\|_{A} \\
          &\leq \left( \min_{p\in \mathbb{P}_{k}, p(0)=1}\max_{\lambda  \in [\lambda_{\min }, \lambda_{\max }]} |p(\lambda)|\right) \left\|(x_{*}-x_{0})\right\|_{A}
          .\end{align*}
        Now we use a change of variable to estimate $  \min\max |p(\lambda)| $. Define: \[
        \mu = 2 \frac{\lambda -\lambda _{\min }}{\lambda _{\max }-\lambda _{\min }} -1.
        \] i.e. $\lambda =\lambda _{\min }\implies \mu =-1 $, $\lambda =\lambda _{\max }\implies \mu =1$. Thus, we estimate: \[
 \min_{p\in \mathbb{P}_{k}, p(\mu_{0})=1}\max_{\mu  \in [-1,1]} |p(\mu)|
        \] where $\mu _{0} = 2 \frac{-\lambda _{\min }}{\lambda _{\max }-\lambda _{\min }} -1 = -\frac{\lambda _{\max }+\lambda _{\min }}{\lambda _{max}-\lambda _{\min }} $.\\

        The solution of the minimax is given by the \vocab{Chebychev polynomial}.\index{Chebchev polynomial}
\end{enumerate}
\begin{lemma}
  If $\mu _{0}\neq [-1,1]$, then: \[
    \frac{C_{k}(\mu)}{C_{k}(\mu_{0})} = \argmin_{p\in \mathbb{P}_{k}, p(\mu_{0})=1}\max_{\mu  \in [-1,1]} |p(\mu)|
  \] where: \[
C_{k}(\mu) = \begin{cases}
 \cos (k\cdot \arccos(\mu)) & \mu  \in [-1,1]\\
 \cosh (k\cdot \arccosh(\mu)) & \mu \geq 1\\
(-1)^{k} \cosh (k\cdot \arccosh(-\mu)) & \mu \leq  1
\end{cases}
  \]
\end{lemma}
\begin{proof}
  First we check that $C_{k} \in  \mathbb{P}_{k}$. Indeed \begin{align*}
                                                            C_{0}(\mu)&=1\in \mathbb{P}_{0}\\
                                                            C_{1}(\mu) &= \mu \in \mathbb{P}_{1}
                                                            .\end{align*} Also, by: \[
\begin{cases}
  \cos ((k+1)\theta ) + \cos ((k-1)\theta) = 2\cos \theta  \cos (k\theta)\\
  \cosh ((k+1)\theta ) + \cosh ((k-1)\theta) = 2\cosh \theta  \cosh (k\theta)\\
\end{cases} \] Choosing $\theta  = \arccos  \mu $ if $|\mu |\leq 1$ or $\arccos h |\mu |$ if $|\mu |\geq 1$ and $k = k+1$, we have:
\begin{align*}
  &C_{k}(\mu)+C_{k-2}(\mu) = 2 \mu C_{k-1}(\mu)\\
  \implies & C_{k}(\mu) = 2\mu C_{k-1}(\mu)-C_{k-2}(\mu) \in \mathbb{P}_{k}
  .\end{align*}
\end{proof}


\end{proof}





\end{document}
